{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cefbbd30",
   "metadata": {},
   "source": [
    "## temp\n",
    "\n",
    "Using '../data/match_event_data.csv'\n",
    "Once the following fields have been manually filled-\n",
    "\n",
    "- Date\t\t\t\t\n",
    "- Events_url\t\n",
    "- 1st_half_file\t\n",
    "- 2nd_half_file\t\n",
    "- 1st_half_start\t\n",
    "- 1st_half_end\t\n",
    "- 2nd_half_start\n",
    "- 2nd_half_end\n",
    "\n",
    "then running this script will automatically fill in the rest using\n",
    "\n",
    "```\n",
    "scrape_events = Scraper()\n",
    "data = scrape_events.fill_match_data_with_event_data()\n",
    "data\n",
    "```\n",
    "\n",
    "if a mistake occurs and an easy fix is needed, it is possible to recover the csv using \n",
    "\n",
    "```\n",
    "scrape_events.backup_csv\n",
    "```\n",
    "\n",
    "NOTE:\n",
    "\n",
    "when manually filling the csv with numbers dont save just export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87975e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary  # Adds chromedriver binary to path\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# Get lineup \n",
    "############################\n",
    "def get_lineup(driver):\n",
    "    current_url = driver.current_url\n",
    "    example_url = \"/lineups/\".join(current_url.rsplit(\"/\",1))\n",
    "    driver.get(example_url)\n",
    "    \n",
    "    com_page = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    match_score = com_page.find_all('div',{'class':'widget-match-header__score'})[0].text\n",
    "    # getting team 1 lineup list\n",
    "    \n",
    "    def get_sub_lineup(div_name):\n",
    "        lineup_block = com_page.find_all('ul',{'class':div_name})\n",
    "        team_list = []\n",
    "        for team in lineup_block:\n",
    "            players = []\n",
    "            names = team.find_all('span', {'class':'widget-match-lineups__name'})\n",
    "            numbers = team.find_all('span', {'class':'widget-match-lineups__number'})\n",
    "            final_names = [item.text for item in names]\n",
    "            final_numbers = [item.text for item in numbers]\n",
    "            for i in range(len(final_numbers)):\n",
    "                players.append([final_numbers[i],final_names[i]])\n",
    "            team_list.append(players)\n",
    "        return team_list[0], team_list[1]\n",
    "    \n",
    "    lineup_name = 'widget-match-lineups__list widget-match-lineups__starting-eleven'\n",
    "    team_home_lineup, team_away_lineup = get_sub_lineup(lineup_name)\n",
    "    \n",
    "    subs_name = 'widget-match-lineups__list widget-match-lineups__substitute'\n",
    "    team_home_sub, team_away_sub = get_sub_lineup(subs_name)\n",
    "    \n",
    "    coaches = [item.text.strip() for item in com_page.find_all('a',{'class': 'widget-match-lineups__manager'})]\n",
    "    return [[team_home_lineup, team_home_sub],[team_away_lineup, team_away_sub],coaches]\n",
    "\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DF = pd.DataFrame(columns=['Date','Home','Away','Score','Home_players','Away_players','Home_subs','Away_subs','Key_events','Commentary'])\n",
    "        self.backup_csv = pd.read_csv('../data/match_event_data.csv')\n",
    "        \n",
    "    def waitforxpath(self,driver,path,return_element=False): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH,path)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.XPATH, path)\n",
    "            \n",
    "        if return_element:\n",
    "            return element\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def waitforselector(self,driver,path): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR,path)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.CSS_SELECTOR, path)\n",
    "        return s\n",
    "            \n",
    "    def waitforname(self,driver,name): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME,name)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.NAME, name)\n",
    "            time.sleep(0.5)\n",
    "        return s\n",
    "    \n",
    "    def waitfortext(self,driver,text): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.LINK_TEXT,text)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.LINK_TEXT, text)\n",
    "            time.sleep(0.5)\n",
    "        return s\n",
    "    \n",
    "    def waitforpartialtext(self,driver,text): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.PARTIAL_LINK_TEXT,text)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.PARTIAL_LINK_TEXT, text)\n",
    "            time.sleep(0.5)\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def waitforclass(self,driver,class_name): \n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME,class_name)))\n",
    "        finally:\n",
    "            s=driver.find_element(By.CLASS_NAME, class_name)\n",
    "            time.sleep(0.5)\n",
    "        return s\n",
    "    \n",
    "\n",
    "    \n",
    "    def open_matches(self,league = None,URL = None):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "        chrome_options.add_argument(\"--headless\")  \n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        if URL != None:\n",
    "            print('opening URL- ',URL)\n",
    "            driver.get(URL)            \n",
    "        else:\n",
    "            driver.get('https://www.goal.com/en-us')\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        # allow cookies\n",
    "        try:\n",
    "            self.waitforxpath(driver,'//*[@id=\"onetrust-accept-btn-handler\"]').click()\n",
    "        except:\n",
    "            print('allow cookies failed')\n",
    "            pass\n",
    "        \n",
    "        if URL != None:\n",
    "            return driver\n",
    "        \n",
    "        # remove newsletter popup\n",
    "        try:\n",
    "            self.waitforselector(driver,'body > div.modal_modal__FGOP8.component-modal.modal_fadeIn__rZptb > div.newsletter-popup_content-wrapper__RgrTF.fade-in > div > div.newsletter-popup_image-wrapper__hqbHD > button').click()\n",
    "        except:\n",
    "            print('remove newsletter failed')\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        # scroll and click on matches\n",
    "        actions = ActionChains(driver)\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", self.waitfortext(driver,'More fixtures'))\n",
    "        self.waitfortext(driver,'More fixtures').click()\n",
    "        \n",
    "    \n",
    "        # open league options and select desired league\n",
    "        self.waitforxpath(driver,'/html/body/div[4]/div[2]/div[1]/div[2]/div[1]/span[1]/span[1]').click()\n",
    "        \n",
    "        if league == 'Premier':\n",
    "            self.waitforpartialtext(driver,\"Premier League\").click()\n",
    "        elif league == 'Champions':\n",
    "            self.waitforxpath(driver,\"//*[contains(@id,'UEFA Champions League')]\").click()\n",
    "\n",
    "        # Matches\n",
    "        self.waitforselector(driver,\"body > div.page-container > div.page-container-bg > div.mr-gutters > div > div > a:nth-child(2)\").click()\n",
    "        \n",
    "        return driver\n",
    "    \n",
    "    \n",
    "    def get_date(self, driver):\n",
    "        DT = self.waitforxpath(driver,\"//*[@class='nav-switch__label']//time\").get_attribute('datetime')\n",
    "        DT = date.fromisoformat(DT[:10])\n",
    "        return DT\n",
    "    \n",
    "    def turn_page(self,driver,direction = -1):\n",
    "        if direction == -1:\n",
    "            self.waitforxpath(driver,\"//*[@class='nav-switch__prev']\").click()\n",
    "        elif direction == 1:\n",
    "            self.waitforxpath(driver,\"//*[@class='nav-switch__next']\").click()\n",
    "        time.sleep(2)\n",
    "        return driver\n",
    "    \n",
    "    def move_to_end_date(self,driver, end_date):\n",
    "        DT = self.get_date(driver)\n",
    "\n",
    "        ii = 0\n",
    "        while DT > end_date:\n",
    "            ii+=1\n",
    "            print(ii, DT)\n",
    "            driver = self.turn_page(driver,-1)\n",
    "            DT = self.get_date(driver)\n",
    "            \n",
    "        \n",
    "        return driver\n",
    "        \n",
    "    \n",
    "    def get_daily_game_list(self,driver):\n",
    "        time.sleep(2)\n",
    "        element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH,\".//span[text()='FT']\")))\n",
    "        game_list = driver.find_elements(By.XPATH,\".//span[text()='FT']\")\n",
    "        print('daily game list length',len(game_list))\n",
    "        return driver,game_list\n",
    "    \n",
    "    def get_match_events(self,driver,path):\n",
    "        time.sleep(2)\n",
    "        element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH,path)))\n",
    "        match_list = driver.find_elements(By.XPATH,path)\n",
    "        print(match_list)\n",
    "\n",
    "    \n",
    "    def get_commentary(self,driver):\n",
    "        comments = []\n",
    "        \n",
    "        com_page = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        comments_block = com_page.find_all('div',{'class':'comment'})\n",
    "        \n",
    "        if comments_block != []:\n",
    "            for idx, comment in enumerate(comments_block):\n",
    "                comments.append(comment.text)\n",
    "                \n",
    "        return comments[::-1]\n",
    "    \n",
    "    def event_widget(self,driver):\n",
    "        element = self.waitforxpath(driver,'.//div[@class=\"btn btn--outline\"]',return_element=True)\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        event_page = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        key_events = event_page.find('div',{'class':'widget-match-key-events in-drawer'}).find_all('div',{'class':'event'})\n",
    "        return driver, event_page, key_events\n",
    "    \n",
    "    \n",
    "    def get_events(self,driver):\n",
    "        #open event widget\n",
    "        element = self.waitforxpath(driver,\"//*[@class='widget-match-key-events']/div[@class='footer']/div[@class='btn btn--outline']\",return_element=True)\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        \n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        success_flag = 0\n",
    "        counter = 0 \n",
    "        while success_flag == 0 and counter < 10:\n",
    "            try:\n",
    "                driver, event_page, key_events = self.event_widget(driver)\n",
    "                success_flag = 1\n",
    "            except:\n",
    "                counter +=1\n",
    "                print('event widget scrape failed', counter)\n",
    "            \n",
    "            \n",
    "        def get_events_per_team(div_name, events):\n",
    "                if div_name == \"event-text team-home clearfix\":\n",
    "                    team_name = \"Home Team\"\n",
    "                else:\n",
    "                    team_name = \"Away Team\"\n",
    "                if event.find_all(\"div\",{\"class\":div_name}):\n",
    "                    time = event.find(\"div\", {'class':'event-time'}).text.strip()\n",
    "                    player = event.find(\"div\", {'class':'event-text-main'}).text.strip()\n",
    "                    additional = event.find(\"div\", {'class':'event-text-additional'}).text.strip()\n",
    "                    if event.find(\"div\", {\"class\": 'match-event-icon type-goal'}):\n",
    "                        action = \"Goal\"\n",
    "                        if additional != \"Goal\":\n",
    "                            events.append([team_name, time, \"Assist\", additional, \"\"])\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": 'match-event-icon type-yellow_card'}):\n",
    "                        action = \"Yellow Card\"\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": 'match-event-icon type-substitution'}):\n",
    "                        action = \"Switch\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": \"match-event-icon type-penalty_goal\"}):\n",
    "                        action = \"Penalty Goal\"\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": \"match-event-icon type-second_yellow_card\"}):\n",
    "                        action = \"Yellow 2nd/RC\"\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": \"match-event-icon type-own_goal\"}):\n",
    "                        action = \"Own Goal\"\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    elif event.find(\"div\", {\"class\": \"match-event-icon type-red_card\"}):\n",
    "                        action = \"Red Card\"\n",
    "                        additional = \"\"\n",
    "                        events.append([team_name, time, action, player, additional])\n",
    "                    else:\n",
    "                        pass\n",
    "                return events\n",
    "                        \n",
    "        if key_events != []:\n",
    "            for idx, event in enumerate(key_events):\n",
    "                home_div = \"event-text team-home clearfix\"\n",
    "                away_div = \"event-text team-away clearfix\"\n",
    "                try:\n",
    "                    events = get_events_per_team(home_div, events)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    events = get_events_per_team(away_div, events)\n",
    "                except:\n",
    "                    pass\n",
    "        print(\"Events done appending\")\n",
    "\n",
    "\n",
    "        reversed_events = events[::-1]\n",
    "\n",
    "        return reversed_events\n",
    "        \n",
    "    def initialize_dataframe(self):\n",
    "        DF = pd.DataFrame(columns=['date','home','away','score','home_team','away_team','home_subs','away_subs','key_events','commentary'])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def GoalScraper(self, league = 'Premier', date_range = ['2022-11-01','2022-11-15']):\n",
    "        start_date = date.fromisoformat(date_range[0])\n",
    "        end_date = date.fromisoformat(date_range[1])\n",
    "        \n",
    "        driver = self.open_matches(league)\n",
    "        driver = self.move_to_end_date(driver,end_date)\n",
    "        \n",
    "        game_date = self.get_date(driver)\n",
    "        while game_date > start_date:\n",
    "            driver, game_list_elements = self.get_daily_game_list(driver)\n",
    "            \n",
    "            print(game_date)\n",
    "            for ii, game in enumerate(game_list_elements):\n",
    "                driver, game_list_elements = self.get_daily_game_list(driver)\n",
    "                # click game\n",
    "                driver.execute_script(\"arguments[0].click();\", game_list_elements[ii])\n",
    "                teams = [team.text for team in driver.find_elements(By.XPATH,\"//*[@class='widget-match-header__name--full']\")]\n",
    "                print(teams)\n",
    "                score = self.waitforxpath(driver,\"//*[@class='widget-match-header__score']\").text\n",
    "                print(score)\n",
    "\n",
    "\n",
    "\n",
    "                events = self.get_events(driver)\n",
    "\n",
    "                commentary = self.get_commentary(driver)\n",
    "\n",
    "                lineup = get_lineup(driver)\n",
    "\n",
    "\n",
    "                # append data to dataframe     \n",
    "                row = {'date':game_date,\n",
    "                      'home':teams[0],\n",
    "                      'away':teams[1],\n",
    "                      'score':score,\n",
    "                      'home_team':lineup[0][0],\n",
    "                      'away_team':lineup[1][0],\n",
    "                      'home_subs':lineup[0][1],\n",
    "                      'away_subs':lineup[1][1],\n",
    "                      'key_events':events,\n",
    "                      'commentary':commentary,\n",
    "                      }\n",
    "\n",
    "                row_df = pd.DataFrame([row])\n",
    "                self.DF = pd.concat([self.DF, row_df], axis=0, ignore_index=True)\n",
    "\n",
    "                driver.back()\n",
    "                driver.back()\n",
    "\n",
    "\n",
    "            driver = self.turn_page(driver,-1)\n",
    "            game_date = self.get_date(driver)\n",
    "        \n",
    "    def GoalScraperFromURL(self,URL):\n",
    "        driver = self.open_matches(URL=URL)\n",
    "        game_datetime = self.waitforxpath(driver,'//h1/time').get_attribute('datetime')\n",
    "        game_datetime = datetime.strptime(game_datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        teams = [team.text for team in driver.find_elements(By.XPATH,\"//*[@class='widget-match-header__name--full']\")]\n",
    "        score = self.waitforxpath(driver,\"//*[@class='widget-match-header__score']\").text\n",
    "        \n",
    "        print(teams, score, game_datetime)\n",
    "\n",
    "        events = self.get_events(driver)\n",
    "\n",
    "        commentary = self.get_commentary(driver)\n",
    "\n",
    "        lineup = get_lineup(driver)\n",
    "        \n",
    "\n",
    "        # append data to dataframe     \n",
    "        row = {'Date':game_datetime,\n",
    "              'Home':teams[0],\n",
    "              'Away':teams[1],\n",
    "              'Score':score,\n",
    "              'Home_players':lineup[0][0],\n",
    "              'Away_players':lineup[1][0],\n",
    "              'Home_subs':lineup[0][1],\n",
    "              'Away_subs':lineup[1][1],\n",
    "              'Key_events':events,\n",
    "              'Commentary':commentary,\n",
    "              }\n",
    "        row_df = pd.DataFrame([row])\n",
    "        self.DF = pd.concat([self.DF, row_df], axis=0, ignore_index=True)\n",
    "        return row\n",
    "        \n",
    "\n",
    "        print('end')\n",
    "        # force stop\n",
    "#         self.waitforxpath(driver,error)\n",
    "\n",
    "\n",
    "    def fill_match_data_with_event_data(self,filepath = '../data/',filename = 'match_event_data.csv'):\n",
    "        \n",
    "        master = pd.read_csv(filepath + filename)\n",
    "\n",
    "        edited_rows = 0\n",
    "        for row in range(len(master)):\n",
    "            \n",
    "            #check if data missing\n",
    "            if not pd.isnull(master['Home_players'][row]):\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            #check if necessary data exists\n",
    "            if pd.isnull(master['Events_url'][row]) \\\n",
    "                and pd.isnull(master['1st_half_file'][row]) and pd.isnull(master['2nd_half_file'][row]) \\\n",
    "                and pd.isnull(master['1st_half_start'][row]) and pd.isnull(master['1st_half_end'][row]) \\\n",
    "                and pd.isnull(master['2nd_half_start'][row]) and pd.isnull(master['2nd_half_end'][row]):\n",
    "                continue\n",
    "                \n",
    "                \n",
    "                \n",
    "            events = self.GoalScraperFromURL(master['Events_url'][row])\n",
    "\n",
    "            for key in events.keys():\n",
    "                master.at[row, key] = events[key]\n",
    "            \n",
    "            edited_rows += 1\n",
    "        \n",
    "        print('rows edited =',edited_rows)        \n",
    "        master.to_csv(filepath + 'match_event_data.csv',index=False)\n",
    "        print('match_event_data file saved')\n",
    "        return master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper = Scraper()\n",
    "# scraper.GoalScraper(league = 'Premier', date_range = ['2022-11-01','2022-11-15'])\n",
    "# scraper.DF\n",
    "# scraper = Scraper()\n",
    "# scraper.GoalScraperFromURL(URL = 'https://www.goal.com/en-us/match/brentford-v-chelsea/7katrhxuegesndba2en673mz8')\n",
    "# scraper.DF\n",
    "\n",
    "scrape_events = Scraper()\n",
    "data = scrape_events.fill_match_data_with_event_data()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86726473",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv('../data/match_event_data.csv')\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce1b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd706c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f12a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a09a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
